{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "In the following, we explain and analyse the properties of bootstrapping methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Experiment setup\n",
    "\n",
    "We again consider a relatively simple prediction task with a relatively small feature set and an impossible prediction task with the same feature set for comparison. \n",
    "We use majority voting and logistic regression as example classifiers like in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_0 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([0, 0])))\n",
    "sampler_1 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([1, 1])))\n",
    "clf_1 = MajorityVoting()\n",
    "clf_2 = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Naive bootstrap method\n",
    "\n",
    "The simplest application of bootstrap principle is to sample $B$ bootstap samples $\\mathcal{Z}_b$, i.e. sample $B$ times from the set $\\{(\\boldsymbol{x}_1, y_1),\\ldots (\\boldsymbol{x}_N, y_N)\\}$ with replacement. For each of these samples $\\mathcal{Z}_b$, we can compute the empirical risk estimator on the entire dataset:\n",
    "\\begin{align*}\n",
    "E_b=\\frac{1}{B}\\cdot\\sum_{b=1}^B\\frac{1}{N}\\cdot\\sum_{i=1}^N  L(y_i, f_b(\\boldsymbol{x}_i)),\n",
    "\\end{align*}\n",
    "where $f_b(\\boldsymbol{x_i})$ is the prediction for $\\boldsymbol{x}_i$ trained on $\\mathcal{Z}_b$.\n",
    "This naive bootstrap estimate on the test error will be too optimistic as we estimate the risk on the set of data that is some weird mixture of training and test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Leave-one-out bootstrap method\n",
    "\n",
    "It is possible to modify the leave-one-out cross-validation scheme such that the training set is not $n-1$ elements but rather a bootstrap sample over these $n-1$ elements. Formally, this requires us to define $n$ different bootstrap distributions but we can do simple rejection sampling. We sample $B$ times from the set $\\{(\\boldsymbol{x}_1, y_1),\\ldots (\\boldsymbol{x}_N, y_N)\\}$ with replacement and reject cases where $(\\boldsymbol{x}_i, y_i)$ is inside the sample. This leads to the following error estimate:\n",
    "\\begin{align*}\n",
    "E_b^*=\\frac{1}{N}\\cdot\\sum_{i=1}^N \\frac{1}{|C_i|}\\cdot \\sum_{b\\in C_i} L(y_i, f_b(\\boldsymbol{x}_i)),\n",
    "\\end{align*}\n",
    "where $C_i$ is the set of indices $b$ such that the bootstrap sample $\\mathcal{Z}_b$ does not contain $(\\boldsymbol{x}_i, y_i)$ and $f_b(\\boldsymbol{x_i})$ is the corresponding prediction for $\\boldsymbol{x}_i$ trained on $\\mathcal{Z}_b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. .632 bootstrap method\n",
    "\n",
    "The leave-one-out bootstrap estimate $E_b^*$ is too conservative as the predictor is trained on the training set which on average consists of a fraction $0.632$ different datapoints from the available sample. \n",
    "The training error\n",
    "\\begin{align*}\n",
    "E_t = \\frac{1}{N}\\cdot\\sum_{i=1}^N L(y_i, f(\\boldsymbol{x}_i)),\n",
    "\\end{align*}\n",
    "where $f_b(\\boldsymbol{x}_i)$ is a predictor for $\\boldsymbol{x}_i$ trained on the entire dataset, is too optimistic.\n",
    "The average \n",
    "\n",
    "\\begin{align*}\n",
    "E_{.632}= 0.632\\cdot E_b^* + 0.368\\cdot E_t \n",
    "\\end{align*}\n",
    "\n",
    "is a reasonable trade-off between both estimates. This is called the .632 bootstrap method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. .632+ bootstrap method\n",
    "\n",
    "The .632 bootstrap method works rather nicely, except for the cases where the training error $E_t$ is zero or close to zero due to overfitting. \n",
    "To counter this issue, we can define a correction by estimating the relative overfitting rate\n",
    "\\begin{align*}\n",
    "R=\\frac{E_b^*-E_t}{\\gamma- E_t},\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "\\gamma = \\frac{1}{N^2}\\cdot\\sum_{i=1}^N \\sum_{j=1}^N L(y_i, f(\\boldsymbol{x}_j))\n",
    "\\end{align*}\n",
    "is the expected loss when we predict $y_i$ based on randomly chosen $\\vec{x}_j$ value using the predictor $f$ trained on the entire dataset.\n",
    "\n",
    "From the overfitting rate we can compute modified weights for combining the errors\n",
    "\\begin{align*}\n",
    "E_{.632+}=w\\cdot E_b^*+ (1-w)\\cdot E_t,\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "w=\\frac{0.632}{1-0.368\\cdot R}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that when there is no overfitting ($E_b^*=E_t$), then the .632+ bootstrap estimate is the same as the .632 estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Comparison of bootstrap methods (<font color='red'>2p</font>)\n",
    "\n",
    "Implement the basic bootstrapping algorithm which draws $n$ samples randomly with replacement from an $n$-element dataset.\n",
    "You can use `DataFrame.sample(n, replace=True)` for that. On top of that, implement all the bootstrap estimates and compare their behaviour on four example cases:\n",
    "* For each data source and algorithm pair, draw around 1000 datasets of size 100.\n",
    "* For each of these datasets, compute $E_b, E_b^*, E_t, E_{.632}, E_{.632+}$.\n",
    "* For each of these datasets, also sample an $n$-element independent test set and compute the hold-out error $E_h$.\n",
    "* Visualise the results by drawing violin plots and boxplots.\n",
    "* Interpret the results. Which of those estimates is closest to $E_h$? Why some estimates are biased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Analysis of prediction stability (<font color='red'>3p</font>)\n",
    "\n",
    "Bootstrapping can be used to study the stability of a learning algorithm:\n",
    "\n",
    "* You can study how much the coefficients of your model vary.\n",
    "* You can study how fragile is your learning algorithm to noise.\n",
    "\n",
    "Let's explore these concepts by studying the stability of polynomial regression models $y\\sim x^2+x+1$ and $y\\sim x^8 + x^7 + \\cdots + x + 1$.\n",
    "\n",
    "* Stability of coefficients (<font color='red'>1p</font>) \n",
    "  * Fit these models on bootstrapped data and observe regression coefficients by drawing corresponding boxplots. \n",
    "  * Study the mean and variance of individual model coefficients. Declare that a coefficient is insignificant and set it to zero when its mean is not more than 3 standard deviations away from zero. \n",
    "  * Interpret the results. Are both models similar?\n",
    "  \n",
    "  \n",
    "* Stability of predictions (<font color='red'>1p</font>) \n",
    "  * Fit these models on bootstrapped data.\n",
    "  * For each learned model, compute a prediction line in the interval $[-2,1]$.\n",
    "  * Draw a faceted plot with facets for models $y\\sim x^2+x+1$ and $y\\sim x^8 + x^7 + \\cdots + x + 1$.\n",
    "  * On each subplot, plot individual prediction lines. Use `alpha=0.5` to make lines semi-transparent.\n",
    "  * Draw also the average prediction line in red on the plot.\n",
    "\n",
    "\n",
    "* Stability against noise (<font color='red'>1p</font>) \n",
    "  * To study robustness against noise, you can add additional Gaussian noise to $y_i$ values of bootstrapped samples     and later estimate how much the mean squared error increased as a consequence. \n",
    "  * The latter should estimate how sensitive is your method to random noise.\n",
    "  * Experiment with different scale values $\\sigma=0.001, 0.01, 0.1, 1$, and visualise the results.\n",
    " \n",
    "  \n",
    "\n",
    "### Remarks\n",
    "* Use the sampler `regr_sampler` as the data source. \n",
    "* Use [sklearn.linear_model.LinearRegression](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) together with \n",
    "[sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to implement polynomial regression:\n",
    "  * First define additional columns $x_2=x^2, \\ldots, x_8=x^8$.\n",
    "  * Then use linear regression to find corresponding coefficients $\\beta_0,\\beta_1,\\ldots, \\beta_8$.\n",
    "* Use [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) to sample the additional Gaussian noise needed in the last part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_sampler(n: int) -> DataFrame:\n",
    "    return (DataFrame(np.random.uniform(low=-2, high=1, size=n), columns=['x']).\n",
    "            assign(y = lambda df: df['x']**2 + df['x'] + np.random.normal(scale=0.3, size=len(df))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc8508c7ef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAetUlEQVR4nO3df5Bd9Xnf8fezK+2KSAJhSWAhaSMTkR8Yg+xsDY5Sj43jBFNVSgJ1CG0B166GqTVOfwVIqd2WNpNAZjKJKxqPbDOBjGtwobYVl46DLRgHx1BWHgkQ2GbN2NYiBZAAGQm00t59+sc9V7p799xzz733/L6f18yO7t57dPd77o/znO/3eb7fY+6OiIhIO0N5N0BERIpNgUJERCIpUIiISCQFChERiaRAISIikRbk3YCkrVixwtetW5d3M0RESmX37t2H3H1l2GOVCxTr1q1jYmIi72aIiJSKmf243WMaehIRkUgKFCIiEkmBQkREIilQiIhIJAUKERGJpEAhIiKRFChEREru8NFp9u5/jcNHp1N5/srNoxARGSRf3fMCNz/wJAuHhjg5O8sdV13M5g2rE/0b6lGIiJTU4aPT3PzAkxw/Ocvr0zMcPznLTQ88mXjPQoFCRKSkpl59k4VDcw/jC4eGmHr1zUT/jgKFiEhJrTn7DE7Ozs657+TsLGvOPiPRv6NAISJSUsuXjHLHVRezaOEQS0cXsGjhEHdcdTHLl4wm+neUzBYRKbHNG1azcf0Kpl59kzVnn5F4kAAFChGR0lu+ZDSVANGQ29CTma01s4fN7Fkz22dmvxeyjZnZp81s0syeNLN3pd2utOuRRUTKJs8exQzw79z9u2a2FNhtZg+5+zNN23wIuCD4uRT4i+DfVGRRjywiUja59Sjc/aC7fze4/TrwLNB6VN4C3ON1jwHLzGxVGu3Jqh5ZRKRsClH1ZGbrgHcCj7c8tBrY3/T7FPODCWa21cwmzGzi5Zdf7qkNWdUji4iUTe6BwsyWAA8A/9rdf9r6cMh/8Xl3uO9w93F3H1+5MvSSrx2F1SNP12ZZPDLc0/OJiFRFroHCzBZSDxJfcPf/HbLJFLC26fc1wIE02tJcj7xoYf1lMXc2bX+UnXteSONPioiUQp5VTwZ8HnjW3f+0zWY7geuC6qfLgCPufjCtNm3esJqvbftVZmfrnZbpmitXISIDL8+qp43APweeMrM9wX3/ARgDcPfPAA8CVwKTwBvAR9Ju1LETNUYXDHOiNnPqvkauIs06ZRGRosotULj7o4TnIJq3ceDj2bSoLqu1U0REyiL3ZHbRRK2dosl4IjKItIRHiLC1UzQZT0QGlXoUMWgynogMMvUoQrT2Hj7+vvUsHBriOKdzF0pwi8igUKBo0dx7aASG7Q9P0jrPTwluERkUGnpqEbaUx8jwENvef0HqFwcRESki9ShatCuPvfbSMa69dCzVi4OIiBSRehQtospjly8Z5ZK1yxQkRGSgqEcRIotLC4qIlIUCRRtpX1pQRKQsNPTUBc3MFpFBpB5FTJqZLSKDSj2KGDQzW0T6VeYRCfUoYmjMrdDMbBHpRdlHJBQoYtDS4yLSq7DVHm564Ek2rl8B1E9EF48Mc+xErbBVlgoUMTTmVtzUckZQxDdURIql3YjEFx7/Cf/jkUkAjp+cZXTYsCErZG9DgSImza0QkV6EjUicqNW48+FJpmdO3z9dc6j5qd5GkY4xSmZ3QTOzRQZXr8nosNUetr3/AkaGww+/jfxnkahH0aXDR6fVqxAZMP0mo1tHJADuDIadWhUx/6lA0YWyVy6ISPeiktHdnCy2rvbQyHvC/BxF0U5CFSiaRPUWkvqwiEi5pFUe39zLUNVTSbT2Fj656UIuOu+sU2/cvgM/ZQib8380l0Kk+tIsjy/LmnIKFIT3Fm798tMsHhmm5s6Hf3kN901MzalQgGKOJYpIslQer0ABhHctAY6dqAFwz2M/mfd/RhcUcyxRRJI36OXxuZbHmtldZvaSmT3d5vH3mdkRM9sT/HwqjXaEdS2j/MzIMJ+9blyJbJEBMsjl8XnPo/hL4IoO2/ytu28Ifm5LoxHNdc6LR4c7bj/rztvPOyuNpoiIFE6uQ0/u/i0zW5dnGxo2b1jNhavOZM/+13jl2An+9Bs/ODUe+eHxNXxpYmpgxydFZLCVIUfxHjPbCxwA/r2772vdwMy2AlsBxsbGevojnaqefu8DPz+w45MiMtjM3fNtQL1H8TV3vyjksTOBWXc/amZXAn/u7hdEPd/4+LhPTEx01YbDR6fZePsujp88nadYtHCIb998uYKCiAwEM9vt7uNhj+Wdo4jk7j9196PB7QeBhWa2Ium/06h6albE9VZERPJQ6EBhZm81Mwtuv5t6ew8n/Xd0vQkRkfbyLo/9IvAd4BfMbMrMPmpmN5rZjcEmVwNPBzmKTwPXeApjZWGrOyphLSJSl3uOImm95CgatDKsiCSpTMeUqBxFGaqeMlOWdVdEpPiqtNp0oXMUIiJl1Lx+3OvTMxw/OctNDzzZ9UWPikKBQkQkYVWrpFSgyECvl1AUkXKqWiWlchQpq9I4pYjEU7WlyRUoUqSr4okMriotTa5AkaK0LqEoIuVQlUpK5ShS1M84pfIaIlIU6lGkqJtxyuaJOY9OHlJeQ0QKQ4EiZXHGKZsT3idqs9RmZ5mZRXkNESkEBYoMRI1ThiW8WymvIdKdMi2dUQYKFDkLS3i3KnP9tUjWVJKePCWzcxaW8F44bIwusK5WslXyW6S/pTP0HWpPPYqctUt4t+Y1orrSOoMSqeu1JF3foWgKFAXQLuHd+DfsQ9zYfvHIsCb1iQR6KUnXxNjOFCgKol3CO+xD/G+/tIfhoSFGhoeYrs1iLdcUUfJbBlUvS2ckPTG2iol0BYqCC/sQz8zCzOws0zPhCXAlv2WQNffQF48Mc+xEjcNHp9setJNcwK+qQ1hKZhdc2Ie41aKFQ4wMd5f8Fqmy5UtG+dHhY2za/ij/7HOPs/H2Xezc80LbbZO4FHLVrkHRTD2KgmvtSp+o1Zh1OFmbO9z04Cf+IcdO1CrV3RXpVbd5hyQW8Kvy2m4KFCXQ+iH+9uSheWOw689dmnczRQqjl4N2vwv4Ve0aFM0UKEqi+UNcpeWLRdKQ10H74+9bz/aHn2NkeLj016BopkBRUlVZvlgkDVlfOKg5iQ3G1veez7WXjlXmO6pAISKVFNXzTrKENSwfcucjk1x76Vhfz1skuQYKM7sL2AS85O4XhTxuwJ8DVwJvADe4+3ezbaWIlFVYzzvpEtYqJ7Eb8i6P/UvgiojHPwRcEPxsBf4igzaJSAWErd2URglrlZPYDbkGCnf/FvBKxCZbgHu87jFgmZmtyqZ1IlJWX93zAhtv3zVvDkXj7L9Z4+y/V0nNwyiyoucoVgP7m36fCu472LyRmW2l3uNgbKw644Ii0r2oORRpnf1XvRIx76GnTizkPp93h/sOdx939/GVK1dm0CwRKaqoXkOaZ//Ll4xyydpllQsSUPwexRSwtun3NcCBnNpSeFVcjEykW516DVU/+09D0QPFTmCbmd0LXAoccfeDHf7PQKrqYmQi3Yozh0LzkLqTd3nsF4H3ASvMbAr4T8BCAHf/DPAg9dLYSerlsR/Jp6XFpvX0ReZSryFZuQYKd//dDo878PGMmlNag1DHLdKtJHoNGs6tK/rQk8QwCHXcIlnTcO5pRa96khgGoY5bJEtVvrZEL9SjqAiNyYokR8O5cylQVIgqOUSSoeHcuTT0JCLSQsO5c6lHISISQsO5pylQiEhbg14e2jycO8ivhQJFRgb5QyblpPLQ0+K+FlX9nitQZEBfOCkbzfY/Le5rUeXvuZLZKVM9tpRRGtdtKKs4r0XVv+cKFCnTF07KKMny0LArzZVJnNei6t9zBYqUqR5byiip8tB2V5orkzivRdW/51Zfd686xsfHfWJiIu9mzLFzzwvzljyuythlmqqaGCyTft6Dw0en2Xj7Lo6fPH0AXbRwiG/ffHlm72eSn6FOz1X277mZ7Xb38bDHlMzOgOqxu1flxGCZ9DPbP+9lMKI+Q70EkE6vRZW/5woUGSnL8hpFOItXxU015DkcE/UZenTyUGonIWX5nndLOYoB15xoLMp4ctUTg4Mi7WUwopLk7T5D+w4cqXR1UlrUoxhgzV3zE7Uasw4na577WXzVE4ODJK3hmE5Dk+0+Q2CxhsOK0LMuEvUoBlRr3ff0jHOyNrewIa+zeC3INlfZy0uXLxnlkrXLEu1JdOoVtPsMvf28MzuehBSlZ10kHXsUZrYN+IK7v5pBeyQjYYnGVnmexVc5MdgNJfXni5skb/cZuuOqi+dVJzWv56T82Hxxhp7eCjxhZt8F7gK+7lWrqR1AYV3zBUMwPDTEyPD8L1AeqpoYjEsHrXDdDE2GfYaiTkLyrtQqqo6Bwt3/o5l9Evh14CPAdjP7EvB5d/9h2g2UdDS65q1nVjqLLw4dtMK1++x285q0OwlRfixcrGS2u7uZ/T3w98AMcDZwv5k95O43pdlASU+7M6tBPggViQ5a7aU1NJlEEKqijjOzzewTwPXAIeBzwFfc/aSZDQHPufvPpd/M+Io4M1ukV2Wf7VtWg1j11O/M7BXAb7v7j5vvdPdZM9uURANFJJyS+vkY9PxYq47lse7+qdYg0fTYs/38cTO7wsy+b2aTZnZLyOM3mNnLZrYn+PlYP39Piqns5Z9pS7q8VKRbuU24M7Nh4E7gg8AU9cqqne7+TMum97n7tswbKJlQ+adI8eU54e7dwKS7P+/uJ4B7gS05tmeg5XFWX/WLvUhn6k2WQ55LeKwG9jf9PgVcGrLdVWb2XuAHwL9x9/2tG5jZVmArwNjYWApNrba8zupV/jnY1Jssjzx7FBZyX2sJ1l8D69z9YuAbwN1hT+TuO9x93N3HV65cmXAzqy2ts/o4Z4oq/xxc6k2WS56BYgpY2/T7GuBA8wbuftjdG5+czwK/nFHbBkYaK7XGXStHazoNriQ/dxq+Sl+eQ09PABeY2duAF4BrgGubNzCzVe5+MPh1M9BXlZXMdfjoNEfePMGJWm3O/SdnZ1k8Msze/a91XZLZ7bITKv8cTEn1JjV8lY3cAoW7zwQLDn4dGAbucvd9ZnYbMOHuO4FPmNlm6rPBXwFuyKu9VdP8BZv1+jpPZyxcwMnZWT48voZN2x+N/PK1m5DUS95BNeuDJ4kZ0EmuhTWIE+y6kev1KNz9QeDBlvs+1XT7D4A/yLpdVRf2BRtdMMSd//RdnHfWIjZtfzTyyxd1Fqe8g8TVb28yqWII9Uo60/UoBlDY+PDI8BBnnbGQYydqkWPHnZKQyjtIN/qZTJjESYmS6vHoCncDqNMXLOqxOGdxyjtIFpIYvlKJdjwKFAOo0xcs6rG4Z3HKO0gW+j0p0VBpPB1Xjy0brR4bX1QCL+qxsBVN1YOQstIKvXVRq8cqUEhPmgPJo5OHlAyUUlPVU//LjIvM0xha0uU6pQo0VBpNVU/SlzRmdotIsShQSF+UDBSpPgUK6YvmTYhUn3IU0rcizZtQUlIkeQoUkoh2ycA0DtztnlNLMYikQ4FCUpPGgbvdc5at+ko9HykTBQpJRRoH7qjnLNNSDOr5SNkomS2pSKNsNuo5y1J9leUidLqgjyRFgUJSkcaBO+o5y1J9ldW8k7hXGRSJQ0NPkorGgfv379/LsA1R8+5X9mz3nO0WLCxS9VU7WfR8ypavkeJToJDU1FcRMzDALZHn7BQMsqy+6kUSS2N3UqZ8jZSDAoWkonFWOz1z+mCV1Fltt+vyFC15nHbPpyz5GikP5SgkFUVZA6qoVzCLc2W3XpPRZcnXSHmoRyGpKMJZ7eGj0zz8vZcYtrnDXlkOw/Q65NVvL6gM+RopDwUK6UmnA2AWY/FRGgfaBUPGsRO1OY9lFbB6Pdh3k4yOeh+0dLYkRYFCuhb3AJjXWW3zgbbZ4pFhau6nAlaaCe5+Ko/iJqOLlnspgqIULVSNAoV0pdsDYB5ntWEH2sWjw/yXf/x23v+L57B8yWjqB9l+Ko/iDNuFvQ+/f/+TLPuZEd5+3pkDeZBU4ExPrslsM7vCzL5vZpNmdkvI46Nmdl/w+ONmti77VkqzoiSpo4QdaGuzfipIZJHg7idHEycZHfY+TM/McuNf7S70BLu0ZosXtWihKnLrUZjZMHAn8EFgCnjCzHa6+zNNm30UeNXd15vZNcDtwO9k31ppKEKSupNO+ZEs5hn0m6PpNGwX9j4AvHGyno9JohQ56WGcNM/4NXckXXkOPb0bmHT35wHM7F5gC9AcKLYA/zm4fT+w3czM3T3LhsppeSep44o60GYV7PrN0UQN2zW/D0NmvNGSsO/3IJn0QT3t2eJlOIEpszwDxWpgf9PvU8Cl7bZx9xkzOwIsBw41b2RmW4GtAGNjY2m1VwJlKb1sd6DNMtilmaNpvA/7DhzhX94zwfTM6fOnfg6SaRzU0z7jL8sJTFnlGSjC1nRo7SnE2QZ33wHsABgfH1dvIwNlL70sS7DrZPmSUd778+fwJ1dfkthBMo2DehZn/FV5T4soz0AxBaxt+n0NcKDNNlNmtgA4C3glm+ZJ1ZU92DVL8iCZxkE9qzP+Kr2nRZJnoHgCuMDM3ga8AFwDXNuyzU7geuA7wNXALuUnyqNsNe1la2+rpA6SaR3UW4MZwN79r5X29R4kuQWKIOewDfg6MAzc5e77zOw2YMLddwKfB/7KzCap9ySuyau90p2y1bSXrb2tGkFu8cgwx07U+j74pjWM0whmZX+9B41V7QR9fHzcJyYm8m7GQDt8dJqNt++aMzN60cIhvn3z5YU8cyxbe1s1Dro+60zXnEUL6/MrinrwLfvrXVVmttvdx8Me0+qxkrg0JuWleVnPMkwibKe5Qmm6Vj/pO35yttATzsr8eg8qLeEhiUs6GZr2MEWZa/DDKpQaijrhrJfXu+z5o7JTj0ISl+T1ELJYmqHM129oN0Mbihvsun29df3v/KlHIalIKhma1dIMZa3Bb65QCstRFHU/4r7euv53MShQSGqSKNfMcliorDX4zQfdpKqeshDn9dYaTsWgQCGF1RiX/uQ/upD/+n+eKdzSDEmOm/f7XGUNcp2UOX9UJQoUUkitCexPbrqQi847qzBnykkm2DWnoD2t4VQMmkchmejmjDmvOvu4bUyyfZpTEE+790bVUMmJmkehHoWkrtsz5jzGpbtpY5Lt0xh8PGFDa+qJZUflsZKqXspbsx6X7raNSbYv7mVP05psWFa6ol22FCgkVb3Mws16XkO3beymfZ0O8p2eS3MIwml2d7Y09CSp6vXsO815DZMvvs6e/a+xYe0y1p+7tKc2xmlf3KGRzRtWc+GqM+e0CTSHIIqqobKlQCGp6qdqJY2Sz0995Snueewnp36/7j1j3LblHT21Map93Rzk2wUU5S/aUzVUthQoJHVFmfU8+eLrc4IEwD3f+QnXXbYu8TbGPchHBZSszpqTXqI8K0X5XA0CBQrJRBEmhO3Z/1rb+9efuzTRNsY9yEcFlEvWLkv9rLnRm4H6qrOjw4YNWaoVREmWtBbhczUIFCikVPo5yGxYu6yr+/sRd2gkKqAcPjrNzy5fzNe2/WoqZ/rNvZmG6ZpDzVPLhaiktZwUKKQ0+j3IrD93Kde9Z4x7vjM3R9FIHnerU9CKMzTSLqA8Onlo3r5eknBAy3qJciXny0uBQkohqYPMbVvewXWXrZtXYdStuEErztBI2LWkG7O10zygZr1EuZLz5aV5FFIKSdbNrz93KVePr+2rJ5H0ZK/lS0a5ZO0yli8ZzWyOQPMcjsbS5KPDltq8FZW0lpd6FFIKRTrIpH1mnOW+ZrlEuUpay0uBQkqhSAeZtA/kWe9rlpVDKmktJ60eK6VSlNVCd+55Yd6BPOnqnaLsqwwGrR4rlVGUuvkszoyLsq8iChQiPdKBXAZFLlVPZvYWM3vIzJ4L/j27zXY1M9sT/OzMup0iIpJfeewtwDfd/QLgm8HvYd509w3Bz+bsmiciIg15BYotwN3B7buB38ypHSJz6CJBIvPllaM4190PArj7QTM7p812i8xsApgB/tjdvxK2kZltBbYCjI2NpdFeGQBah0gkXGqBwsy+Abw15KFbu3iaMXc/YGbnA7vM7Cl3/2HrRu6+A9gB9fLYnhosAy3rdYhU+iplklqgcPdfa/eYmb1oZquC3sQq4KU2z3Eg+Pd5M3sEeCcwL1CIRIlzUM5yHSL1XKRs8spR7ASuD25fD3y1dQMzO9vMRoPbK4CNwDOZtVAqIe41p7O8SFDS60SJpC2vQPHHwAfN7Dngg8HvmNm4mX0u2OaXgAkz2ws8TD1HoUAhsXVzUG5eIG/p6ILUFsbLasE/kSTlksx298PAB0LunwA+Ftz+O+AdGTdNKqTb4aQsZlsXaXFDkbi0zLhUVi8H5eblvtOQVc9FJElawkMqq0grzjbTCqpSNgoUUmlFPShrnSgpEwUKqTwdlEX6oxyFiIhEUqAQEZFIChQiIhJJgUJERCIpUIiISCQFChERiaRAISIikRQoREQkkgKFiIhEUqAQSYmuvy1VoSU8RFKgq9hJlahHIZIwXcVOqkaBQiRhRbyKnYbBpB8aehJJWNGuYqdhMOmXehQiCSvSVew0DCZJUI9CJAVFuWBSt9cNFwmjQCGSkiJcMKlow2BSThp6EqmwIg2DSXmpRyFScUUZBpPyyqVHYWb/xMz2mdmsmY1HbHeFmX3fzCbN7JYs2yhSJcuXjHLJ2mUKEtKTvIaengZ+G/hWuw3MbBi4E/gQcCHwu2Z2YTbNExGRhlyGntz9WQAzi9rs3cCkuz8fbHsvsAV4JvUGiojIKUVOZq8G9jf9PhXcN4+ZbTWzCTObePnllzNpnIjIoEitR2Fm3wDeGvLQre7+1ThPEXKfh23o7juAHQDj4+Oh24iISG9SCxTu/mt9PsUUsLbp9zXAgT6fU0REulTkoacngAvM7G1mNgJcA+zMuU0iIgPH3LMfqTGz3wL+O7ASeA3Y4+6/YWbnAZ9z9yuD7a4E/gwYBu5y9z+M8dwvAz/uoVkrgEM9/L8i0r4Uk/almLQvdT/r7ivDHsglUBSRmU24e9s5HWWifSkm7UsxaV86K/LQk4iIFIAChYiIRFKgOG1H3g1IkPalmLQvxaR96UA5ChERiaQehYiIRFKgEBGRSAMbKMzsT8zse2b2pJl92cyWtdmu8Eudd7Fs+4/M7Ckz22NmE1m2Ma4qLUFvZm8xs4fM7Lng37PbbFcL3pM9ZlaoSaWdXmczGzWz+4LHHzezddm3Mp4Y+3KDmb3c9F58LI92dmJmd5nZS2b2dJvHzcw+Heznk2b2rr7/qLsP5A/w68CC4PbtwO0h2wwDPwTOB0aAvcCFebc9pJ2/BPwC8AgwHrHdj4AVebe3330p0ftyB3BLcPuWsM9Y8NjRvNva6+sM/CvgM8Hta4D78m53H/tyA7A977bG2Jf3Au8Cnm7z+JXA/6W+Xt5lwOP9/s2B7VG4+9+4+0zw62PU15JqdWqpc3c/ATSWOi8Ud3/W3b+fdzuSEHNfSvG+UG/T3cHtu4HfzLEtvYjzOjfv4/3AB6zD9QNyUpbPTEfu/i3glYhNtgD3eN1jwDIzW9XP3xzYQNHiX1CPwK1iL3VeEg78jZntNrOteTemD2V5X85194MAwb/ntNluUbBM/mNmVqRgEud1PrVNcOJ1BFieSeu6E/czc1UwXHO/ma0NebwMEv9+VPqa2XGWOjezW4EZ4AthTxFyXy71xAks2w6w0d0PmNk5wENm9r3g7CRTWS5Bn7aofeniacaC9+V8YJeZPeXuP0ymhX2J8zoX5r3oIE47/xr4ortPm9mN1HtKl6fesuQl/p5UOlB4h6XOzex6YBPwAQ8G91oUZqnzTvsS8zkOBP++ZGZfpt4dzzxQJLAvpXhfzOxFM1vl7geDrv9LbZ6j8b48b2aPAO+kPp6etzivc2ObKTNbAJxF9LBIXjrui7sfbvr1s9Rzl2WU+PdjYIeezOwK4GZgs7u/0Wazyix1bmaLzWxp4zb1ZH5o1UQJlOV92QlcH9y+HpjXWzKzs81sNLi9AthIcS73G+d1bt7Hq4FdbU668tZxX1rG8TcDz2bYviTtBK4Lqp8uA440hkB7lncGP68fYJL6ON6e4KdRuXEe8GBLBcEPqJ/h3Zp3u9vsy29RP4uYBl4Evt66L9SrPfYGP/vKvC8lel+WA98Engv+fUtw/zj15fQBfgV4KnhfngI+mne7W/Zh3usM3Eb9BAtgEfC/gu/T/wPOz7vNfezLHwXfjb3Aw8Av5t3mNvvxReAgcDL4rnwUuBG4MXjcgDuD/XyKiErIuD9awkNERCIN7NCTiIjEo0AhIiKRFChERCSSAoWIiERSoBARkUgKFCIiEkmBQkREIilQiKTMzP5BsNDcomCG/D4zuyjvdonEpQl3Ihkws/9GfRbzGcCUu/9Rzk0SiU2BQiQDwfpCTwDHgV9x91rOTRKJTUNPItl4C7AEWEq9ZyFSGupRiGQguBb2vcDbgFXuvi3nJonEVunrUYgUgZldB8y4+/80s2Hg78zscnfflXfbROJQj0JERCIpRyEiIpEUKEREJJIChYiIRFKgEBGRSAoUIiISSYFCREQiKVCIiEik/w9POBgdUonUNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regr_sampler(100).plot(x = 'x', y = 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Bootstrap and its reduction to Monte Carlo cross-validation* (<font color='red'>3p</font>)\n",
    "\n",
    "Note that the same datapoint can occur many times in a bootstrap sample. \n",
    "This does not play nicely with methods that ignore the sample count in training, such as Support Vector Machines that fix the decision border based on the support vectors alone. Study this phenomenon by considering a simple linear classification task. Compute leave-one-out bootstrap estimate $E_b^*$ and compare it with error estimate $E_{mc}$ of Monte Carlo cross-validation scheme with a ratio $0.632:0.368$ between training and test set.\n",
    "\n",
    "* Do both methods obtain the same average error estimate? \n",
    "* If not, which of them is closer to the true test error for the model trained over the entire dataset?\n",
    "* Estimate the variances of $E_b^*$ and $E_{mc}$. Are they comparable?\n",
    "* Is there a difference if you consider a hard SVM that ignores multiplicity and soft SVM that considers multiplicity through hinge loss? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
