{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation and bias in hyperparameter selection\n",
    "\n",
    "Many classification algorithms use cross-validation error to choose hyperparameters.\n",
    "If hyperparameters strongly depend on the dataset size then comparison of cross-validation errors lead to incorrect parameter choices if the fold count $k$ is small.\n",
    "On the other hand, leave-one-out cross-validation scheme is not very useful for very stable classification methods for which the classification does not change if we drop a single point, such as support vector machines.\n",
    "This creates a trade-off. In the following, we explore this phenomenon in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Bias in hyperparameter estimation* (<font color='red'>3p</font>)\n",
    "\n",
    "Use a large dataset, such as [UCI: Diabetes 130-US hospitals for years 1999-2008 Data Set](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008#) to test this issue: \n",
    "\n",
    "* Bias detection (<font color='red'>1p</font>)\n",
    "  * Split the dataset into training and test set.\n",
    "  * Use [SVM with RBF kernel](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) for predicing the target value.\n",
    "  * Use cross-validation to find the best kernel width $\\gamma$ and regularisation parameter $C$.\n",
    "  * Use test set for finding the best kernel width $\\gamma$ and regularisation parameter $C$.\n",
    "  * Compare the results. Is the optimal value of the kernel width $\\gamma$ different? (<font color='red'>1p</font>)\n",
    "\n",
    "* Measure the effect of trade-offs (<font color='red'>1p</font>)\n",
    "  * Use the same setup but try different number of folds. \n",
    "  * Tabulate the discrepancies in optimal hyperparameter values for different fold counts.\n",
    "  * Tabulate the difference in the accuracy.\n",
    "  * Interpret results. What seems to be the best cross-validation scheme?\n",
    "  \n",
    "* Measure the effect of training set size (<font color='red'>1p</font>)\n",
    "  * Choose you favorite fold count $k\\leq 10$ and alter the size of the training set.\n",
    "  * Tabulate the discrepancies in optimal hyperparameter values for different training sets.\n",
    "  * Tabulate the differences in the accuracy.\n",
    "  * Interpret results. Does this effect diminish when the training set increases?\n",
    "\n",
    "\n",
    "### Remarks\n",
    "* Any dataset where SVM with RBF kernel works and contains more than 3000 data points is suitable. The hold-out set must be large enough to get a reliable test error estimate.\n",
    "* RBF kernel width is known to be dependent on the sample density. If density is low, the width must be large or otherwise the SVM collapses to nearest neighbour classifier. The density obviously increases if we add back a large fraction of datapoints and hence the optimal $\\gamma$ value should decrease. \n",
    "* As cross-validation can be computationally quite demanding, you can use a simple hold-out split for hyperparameter tuning to cut corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
