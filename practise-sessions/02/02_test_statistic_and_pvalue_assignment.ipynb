{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test statistic and p-value assignment\n",
    "\n",
    "Recall that any fixed classification rule that does not use a training set to tune its parameters can be converted to a statistical test. For that we must fix a null hypothesis as a source of negative samples and measure the false positive rate.\n",
    "Usually, a classifier internally computes a decision value $t$ aka test score. Thus we can assign a false positive rate $p(t)$ for each threshold value $t$. This value is known as p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy.random as rnd\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm import tnrange\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Formal definition of a p-value\n",
    "\n",
    "Let $t$ be a test statistic computed from a sample $\\boldsymbol{x}$ as $f(\\boldsymbol{x})$. Let $\\mathcal{D}$ be the distribution of $\\boldsymbol{x}$ determined by the null hypotesis. Then p-value is a probability\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{pvalue}(t)=\\Pr[\\boldsymbol{x}_*\\gets\\mathcal{D}, t_*=f(\\boldsymbol{x}_*): t_*\\geq t]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that this definition corresponds to a classifier that classifies all samples $\\boldsymbol{x}$ as negatives if $f(\\boldsymbol{x})\\geq t$ and $\\mathrm{pvalue}(t)$ is just the fraction of false positives.\n",
    "\n",
    "Obviously, we can reverse the classification rule and then  p-value is a probability\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{pvalue}(t)=\\Pr[\\boldsymbol{x}_*\\gets\\mathcal{D}, t_*=f(\\boldsymbol{x}_*): t_*\\leq t]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Both definitions are quite common. Intuitively, you should fix the direction based on the extremeness. \n",
    "If you consider larger values more extreme then you should consider the first formula. It is also possible to use a double threshold and consider a cut-off based on $|t|$. This leads to the third formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{pvalue}(t)=\\Pr[\\boldsymbol{x}_*\\gets\\mathcal{D}, t_*=f(\\boldsymbol{x}_*): |t_*|\\geq |t|]\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical power of a test\n",
    "\n",
    "Different classification algorithms are good for different tasks. The same is true for statistical tests.\n",
    "Thresholding based on p-value allows us to control the ratio of false positives (significance level).\n",
    "At the same time, it also affects the ratio of false negatives. The latter is harder to quantify as it depends on two factors:\n",
    "* definition of a test statistic (*statistical test*)\n",
    "* the data distribution of positive cases (*alternative hypothesis*)\n",
    "\n",
    "Of course, a test cannot really work equally good for all alternative hypotheses.\n",
    "Hence, a statistical test is commonly defined to work well for all reasonable alternative hypotheses.\n",
    "For instance, let the null hypothesis be that the iid (independent and identically distributed) data sample is from a normal distibution $\\mathcal{N}(\\mu=0,\\sigma=1)$. Then it makes sense to consider a class of alternative hypotheses where the iid data sample is from a normal distribution $\\mathcal{N}(\\mu\\neq 0,\\sigma=1)$. \n",
    "This situation naturally arises in quality control where we must check that some physical quantity is zero and the measurement procedure is corrupted by additive Gaussian noise $\\mathcal{N}(\\mu=0,\\sigma=1)$. \n",
    "\n",
    "Now for any alternative hypothesis specified by a distribution $\\mathcal{D}$, we can compute the recall probability as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[data\\gets \\mathcal{D}: \\text{test accepts }]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "This is known as **power** of the statistical test. A good test has large recall for alternative hypotheses. \n",
    "For our example case, the latter cannot be achieved since $\\mathcal{N}(\\mu\\neq 0,\\sigma=1)$ can be arbitrarily close to \n",
    "$\\mathcal{N}(\\mu=0,\\sigma=1)$. In general, the best we can achieve is that for all the alternative hypotheses, our test performs roughly as well as the best test designed for that null hypothesis and alternative hypothesis pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Properties of p-values (<font color='red'>1p</font>)\n",
    "\n",
    "Let the test statistic $t$ be $t=\\sin(x)$. Find the p-value function $\\mathrm{pvalue}(t)$ for the null hypothesis where $x$ is sampled uniformly form the range $[-\\pi, \\pi]$. Find out what is the distribution of $\\mathrm{pvalue}(t)$ for $t=\\sin(x)$. Explain why you get this result?\n",
    "* You can use simulations or simple probability computations to determine when $\\sin(x_*)\\geq t$.\n",
    "* You can use simulations or simple probability computations to determine the distribution of $\\mathrm{pvalue}(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?rnd.uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(??)+ geom_jitter(aes(y='??', x=0), height=0, width=0.1, alpha=0.3) + xlim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(??) + geom_histogram(aes(x='??'), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Power of a statistical test (<font color='red'>1p</font>)\n",
    "\n",
    "Consider two statistical tests defined by the test statistics $t_1=\\frac{1}{|x|}$ and $t_2=|x|$. \n",
    "Find the p-value functions $\\mathrm{pvalue}_i(t)$ for the null hypothesis where $x$ is sampled form $\\mathcal{N}(\\mu=0, \\sigma=1)$.\n",
    "As an alternative hypothesis, consider the case that $x$ is sampled from $\\mathcal{N}(\\mu\\neq 0, \\sigma=1)$.\n",
    "For simplicity, you can fix $\\mu=1$.\n",
    "Compute the power of both tests at the significance level $5\\%$.\n",
    "You can do simulations but there exists a simple  closed form solution for this.\n",
    "\n",
    "**Clarification:** Use the one-sided formula $\\mathrm{pvalue}(t)=\\Pr[\\boldsymbol{x}_*\\gets\\mathcal{D}, t_*=f(\\boldsymbol{x}_*): t_*\\geq t]$\n",
    "for computing pvalues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?rnd.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?stats.norm.ppf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?stats.norm.cdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
