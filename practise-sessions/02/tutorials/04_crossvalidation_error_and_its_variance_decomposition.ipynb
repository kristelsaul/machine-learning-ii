{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation error and its variance decomposition\n",
    "\n",
    "Cross-validation error as any empirical risk estimator depends on the observed data.\n",
    "The magnitude of statistical fluctuations is characterised by its variance â€“ we can expect that for large enough datasets, the cross-validation error is distributed similarly to a normal distribution.\n",
    "\n",
    "The paper *No Unbiased Estimator for the Variance of K-Fold Cross-Validation* by *Yoshua Bengio* and *Yves Grandvalet* provides a nice decomposition result for the variance of cross-validation error\n",
    "\\begin{align*}\n",
    "\\theta= \\frac{1}{n}\\cdot\\sigma^2+ \\frac{m-1}{n}\\cdot \\omega + \\frac{n-m}{n}\\cdot\\gamma,\n",
    "\\end{align*}\n",
    "where\n",
    "* $\\sigma^2$ is the average variance of a true test error \n",
    "* $\\omega$ is within-block covariance of test errors\n",
    "* $\\gamma$ is between-block covariance of test errors\n",
    "\n",
    "As it is quite difficult to formally define without specifying an algorithm, we define these in terms of Python code instead of a formal definition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm.notebook import tnrange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from common import logit\n",
    "from common import data_sampler\n",
    "from common import MajorityVoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Experiment setup\n",
    "\n",
    "We again consider a relatively simple prediction task $\\mathcal{D}_1$ with a relatively small feature set and an impossible prediction task $\\mathcal{D}_0$ with the same feature set for comparison. \n",
    "We use majority voting and logistic regression as example classifiers as in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_0 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([0, 0])))\n",
    "sampler_1 = lambda n: data_sampler(n, 8, lambda x: logit(x, Series([10, 10])))\n",
    "clf_1 = MajorityVoting()\n",
    "clf_2 = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Average variance of a true test error\n",
    "\n",
    "Recall that k-fold cross-validation splits the data into $k$ folds, each of size $m$.\n",
    "The true variance $\\sigma^2$ measures the variance of a test error averaged over all possible training sets of size $(k-1)m$. Note that this term is well defined due to symmetry:\n",
    "\n",
    "* All splits are equivalent.\n",
    "* It does not matter which datapoints form the test set we take.\n",
    "\n",
    "The following algorithm would provide the true value for $\\sigma^2$ in the process $r\\to\\infty$ with probability 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_error_variance(sampler, clf, features, target, n: int, k:int=10, r:int=1000):\n",
    "    assert n % k == 0,  'Crossvalidation is unimplemented for cases n != k * m'\n",
    "   \n",
    "    m = int(n/k)\n",
    "\n",
    "    # Record losses of a data point in a test fold over all possible training sets \n",
    "    loss = Series(np.nan, index=range(r))\n",
    "    for i in tnrange(r):\n",
    "        \n",
    "        # Train a model\n",
    "        train = sampler((k-1) * m)\n",
    "        test  = sampler(m)\n",
    "        clf.fit(train[features], train[target])\n",
    "    \n",
    "        # We record the variance of the first test sample \n",
    "        loss[i] = float((clf.predict(test[features]) != test[target])[0])\n",
    "    \n",
    "    \n",
    "    return loss.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True variance component  for $\\mathcal{D}_0$ and majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560b5f135c8142ae98a2f20551ddd944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b7a8f234ff484abe4b9da96a6092fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma^2 for n = 100\n",
      "+0.250225\n",
      "+0.250234\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_0(1).columns.values[:-1])\n",
    "var_1 = true_error_variance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "var_2 = true_error_variance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "print('Sigma^2 for n = {}\\n{:+.6}\\n{:+.6}'.format(n, var_1, var_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True variance component  for $\\mathcal{D}_1$ and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add92d682b06427ea80c8856da0b3ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f1ba667f924c12b26367f5e4399181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma^2 for n = 100\n",
      "+0.200024\n",
      "+0.189674\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_1(1).columns.values[:-1])\n",
    "var_1 = true_error_variance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "var_2 = true_error_variance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "print('Sigma^2 for n = {}\\n{:+.6}\\n{:+.6}'.format(n, var_1, var_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Within-block covariance\n",
    "\n",
    "Within-block covariance $\\omega$ measures the covariance of two different error terms in the same test fold.\n",
    "Again, the covariance value is computed over all possible cross-validation sets.\n",
    "Note that this term is well defined due to symmetry:\n",
    "* All splits are equivalent.\n",
    "* It does not matter which datapoints form the test set we take.\n",
    "\n",
    "The following algorithm would provide the true value for $\\omega$ in the process $r\\to\\infty$ with probability 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_block_covariance(sampler, clf, features, target, n: int, k:int=10, r:int=1000):\n",
    "    assert n % k == 0,  'Crossvalidation is unimplemented for cases n != k * m'\n",
    "   \n",
    "    m = int(n/k)\n",
    "    \n",
    "    # There is only one error \n",
    "    if m <= 1:\n",
    "        return 0\n",
    "\n",
    "    # Record losses of two different data points belonging to the same test fold over all the training sets. \n",
    "    loss_1 = Series(np.nan, index=range(r))\n",
    "    loss_2 = Series(np.nan, index=range(r))\n",
    "    for i in tnrange(r):\n",
    "        \n",
    "        # Train a model\n",
    "        train = sampler((k-1) * m)\n",
    "        test = sampler(m)\n",
    "        clf.fit(train[features], train[target])\n",
    "    \n",
    "        # We record the loss of the first and the second test sample in the split\n",
    "        yp = clf.predict(test[features])\n",
    "        loss_1[i] = float((yp != test[target])[0])\n",
    "        loss_2[i] = float((yp != test[target])[1])\n",
    "                \n",
    "    # Compute the covariance between loss vectors              \n",
    "    return loss_1.cov(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within-block covariance component  for $\\mathcal{D}_0$ and majority voting\n",
    "\n",
    "Within-block covariance component $\\omega$ must be zero for all prediction algorithms as true target values are independent, so the errors are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19715162b65a42f7b38b289aa481ab35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8907ffd6292f43ef9f72f5a0509ed0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega for n = 100\n",
      "+0.00446847\n",
      "+0.0107267\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_0(1).columns.values[:-1])\n",
    "omega_1 = within_block_covariance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "omega_2 = within_block_covariance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "print('Omega for n = {}\\n{:+.6}\\n{:+.6}'.format(n, omega_1, omega_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within-block covariance component  for $\\mathcal{D}_1$ and logistic regression\n",
    "\n",
    "As logistic regression quickly learns the true model on training set, test set errors become independent again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d04996a15104dc48daae01be10c2c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e539a4f98f4704a4d9e015e65a2293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega for n = 100\n",
      "-0.0104424\n",
      "+0.00511712\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_0(1).columns.values[:-1])\n",
    "omega_1 = within_block_covariance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "omega_2 = within_block_covariance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "print('Omega for n = {}\\n{:+.6}\\n{:+.6}'.format(n, omega_1, omega_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Between-block covariance\n",
    "\n",
    "Between-block covariance measures the covariance of two error terms in different test folds.\n",
    "Again, the covariance value is computed over all possible cross-validation sets.\n",
    "Note that this term is well defined due to symmetry:\n",
    "* All split pairs are equivalent.\n",
    "* It does not matter which datapoints form the test set we take.\n",
    "\n",
    "The following algorithm would provide the true value for $\\gamma$ in the process $r\\to\\infty$ with probability 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_block_covariance(sampler, clf, features, target, n: int, k:int=10, r:int=100):\n",
    "    assert n % k == 0,  'Crossvalidation is unimplemented for cases n != k * m'\n",
    "   \n",
    "    m = int(n/k)\n",
    "\n",
    "    # Record losses of two data points belonging to different test folds over all the training sets. \n",
    "    loss_1 = Series(np.nan, index=range(r))\n",
    "    loss_2 = Series(np.nan, index=range(r))\n",
    "    for i in tnrange(r):\n",
    "        \n",
    "        # As two splits are overlapping they have joint training set of size (k-2)*m \n",
    "        joint_train = sampler((k-2) * m)\n",
    "        test_1 = sampler(m)\n",
    "        test_2 = sampler(m)\n",
    "        \n",
    "        # We record the loss of the first test sample in the first split\n",
    "        train = pd.concat([joint_train, test_2])\n",
    "        clf.fit(train[features], train[target])\n",
    "        loss_1[i] = float(clf.predict(test_1[features])[0] != test_1[target][0])\n",
    "\n",
    "        # We record the loss of the first test sample in the second split\n",
    "        train = pd.concat([joint_train, test_1])\n",
    "        clf.fit(train[features], train[target])\n",
    "        loss_2[i] = float(clf.predict(test_2[features])[0] !=test_2[target][0])\n",
    "    \n",
    "    # Compute the covariance between loss vectors  \n",
    "    return loss_1.cov(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between-block covariance component for $\\mathcal{D}_0$ and majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f8a19963774e838c221d54d9d86ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd6ac984ef443b59993fbc24500cde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for n = 100\n",
      "+0.00200901\n",
      "-0.00456456\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_0(1).columns.values[:-1])\n",
    "gamma_1 = between_block_covariance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "gamma_2 = between_block_covariance(sampler_0, clf_1, features, 'y', n, k, r)\n",
    "print('Gamma for n = {}\\n{:+.6}\\n{:+.6}'.format(n, gamma_1, gamma_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between-block covariance component for $\\mathcal{D}_1$ and logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff551af478b449339803940ad81f6f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223b83a47a054bcd99aa72ce9009b664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma for n = 100\n",
      "+0.00726727\n",
      "-0.000694695\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = 100\n",
    "r = 1000\n",
    "features = list(sampler_0(1).columns.values[:-1])\n",
    "gamma_1 = between_block_covariance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "gamma_2 = between_block_covariance(sampler_1, clf_2, features, 'y', n, k, r)\n",
    "print('Gamma for n = {}\\n{:+.6}\\n{:+.6}'.format(n, gamma_1, gamma_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Empirical test for the theorem\n",
    "\n",
    "To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of cross-validation variance decomposition* \n",
    "\n",
    "The examples above were not very convincing as $\\omega$ and $\\gamma$ values were too close to zero. \n",
    "Find examples where this is not the case. You can take the paper *No Unbiased Estimator for the Variance of K-Fold Cross-Validation* as a starting point since it contains several examples of such datasets, both artificial and real.\n",
    "Implement either their artificial samplers or find a UCI repository dataset that is similar to the **Letter** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
